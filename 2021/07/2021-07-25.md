### Date

### Note
- 关于模型训练调参的问题。灵活，凭借直觉？
- 确保网络观测到充足的信息。
- 数学需下苦功夫。
- model zoo
- 强化学习算法家族： Q learn Deep Q learn 
- 直接选择行为： policy gradient
- model based RL
- value based and Policy based。
- online learn and off line learn.
- 有定性走向定量
- Bellman贪婪对于未来期望的估值令人深刻，但其作用与线性期望。对于非线性期望，最大期望需在突大点倒退求得。同时也给我们一些启示，短期最优解未必是最优。
- 对于强化学习，需要仔细设计的是指导（基于价值），状态，记忆，学习。
- sarsa算法，与Qlearn的区别在于对Q表的更新方式。二者更新一方面是对收益的滤波吸收，可以理解为吸收与遗忘。但同时还有对该行为的未来收益进行预估，而Q learn是将该行为执行后的状态最大收益作为评估值，而
- 值得注意的是，再更新Q table的过程中reward和Q值被同等对待。值得商榷。