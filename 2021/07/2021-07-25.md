### Date

### Note
- 关于模型训练调参的问题。灵活，凭借直觉？
- 确保网络观测到充足的信息。
- 数学需下苦功夫。
- model zoo
- 强化学习算法家族： Q learn Deep Q learn 
- 直接选择行为： policy gradient
- model based RL
- value based and Policy based。
- online learn and off line learn.
- 有定性走向定量
- Bellman贪婪对于未来期望的估值令人深刻，但其作用与线性期望。对于非线性期望，最大期望需在突大点倒退求得。同时也给我们一些启示，短期最优解未必是最优。
- 对于强化学习，需要仔细设计的是指导（基于价值），状态，记忆，学习。
- sarsa算法，与Qlearn的区别在于对Q表的更新方式。二者更新一方面是对收益的滤波吸收，可以理解为吸收与遗忘。但同时还有对该行为的未来收益进行预估，而Q learn是将该行为执行后的状态最大Q作为评估值，而sarsa则是将同样的行为决策方式作用于新状态的Q作为评估值，即根据E贪婪，有概率是最大Q，有概率是随机行为。 或许也可以将加权平均值作为评估。总而言之，在对未来Q的评估上，Q learn稍稍激进。大多数情况下是合理的，针对于一些Q值有巨大区别的，获取稍有不同。值得注意的是，Q值的意义在于相互比较，其值的大小意义建立在如何再采取相同更新模式下的让优秀的行为取得优势，让好坏产生区分。倘若要定义Q值的具体值，可能要考虑一些数学
- 值得注意的是，再更新Q table的过程中reward和Q值被同等对待。值得商榷。